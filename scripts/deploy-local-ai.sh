#!/bin/bash

# YYCÂ³ AIå°è¯­æ™ºèƒ½æˆé•¿å®ˆæŠ¤ç³»ç»Ÿ - æœ¬åœ°AIæœåŠ¡éƒ¨ç½²è„šæœ¬
# Intelligent Pluggable Mobile AI System - Local AI Services Deployment Script
# Phase 2 Week 9-10: æœ¬åœ°AIæ¨¡å‹é›†æˆ

set -e

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
NC='\033[0m' # No Color

# æ—¥å¿—å‡½æ•°
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

log_step() {
    echo -e "${PURPLE}[STEP]${NC} $1"
}

# é…ç½®å˜é‡
DEFAULT_OLLAMA_MODEL="llama3.1:8b"
DEFAULT_ENVIRONMENT="production"

# æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
show_help() {
    echo "YYCÂ³ æœ¬åœ°AIæœåŠ¡éƒ¨ç½²ç®¡ç†è„šæœ¬"
    echo ""
    echo "ç”¨æ³•: $0 {command} [options]"
    echo ""
    echo "å‘½ä»¤:"
    echo "  deploy [env]        éƒ¨ç½²æœ¬åœ°AIæœåŠ¡"
    echo "  update [env]        æ›´æ–°æœ¬åœ°AIæœåŠ¡"
    echo "  stop [env]          åœæ­¢æœ¬åœ°AIæœåŠ¡"
    echo "  restart [env]       é‡å¯æœ¬åœ°AIæœåŠ¡"
    echo "  status [env]        æ˜¾ç¤ºæœåŠ¡çŠ¶æ€"
    echo "  logs [service]      æŸ¥çœ‹æœåŠ¡æ—¥å¿—"
    echo "  models              ç®¡ç†AIæ¨¡å‹"
    echo "  knowledge           ç®¡ç†çŸ¥è¯†åº“"
    echo "  monitor             ç›‘æ§é¢æ¿"
    echo "  cleanup [env]       æ¸…ç†æœåŠ¡"
    echo "  health [env]        å¥åº·æ£€æŸ¥"
    echo ""
    echo "ç¯å¢ƒ:"
    echo "  production          ç”Ÿäº§ç¯å¢ƒ"
    echo "  development         å¼€å‘ç¯å¢ƒ(åŒ…å«è°ƒè¯•å·¥å…·)"
    echo ""
    echo "ç¤ºä¾‹:"
    echo "  $0 deploy production    # éƒ¨ç½²ç”Ÿäº§ç¯å¢ƒ"
    echo "  $0 pull llama3.1:8b     # ä¸‹è½½æ¨¡å‹"
    echo "  $0 status production    # æŸ¥çœ‹çŠ¶æ€"
    echo "  $0 logs ollama           # æŸ¥çœ‹Ollamaæ—¥å¿—"
    echo ""
    echo "æ¨¡å‹ç®¡ç†:"
    echo "  pull <model>            ä¸‹è½½æŒ‡å®šæ¨¡å‹"
    echo "  list                    åˆ—å‡ºå¯ç”¨æ¨¡å‹"
    echo "  delete <model>          åˆ é™¤æŒ‡å®šæ¨¡å‹"
    echo "  switch <model>          åˆ‡æ¢å½“å‰æ¨¡å‹"
}

# æ£€æŸ¥ä¾èµ–
check_dependencies() {
    log_info "æ£€æŸ¥ç³»ç»Ÿä¾èµ–..."

    # æ£€æŸ¥Docker
    if ! command -v docker &> /dev/null; then
        log_error "Docker æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£… Docker"
        exit 1
    fi

    # æ£€æŸ¥Docker Compose
    if ! command -v docker-compose &> /dev/null; then
        log_error "Docker Compose æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£… Docker Compose"
        exit 1
    fi

    # æ£€æŸ¥ç½‘ç»œè¿æ¥
    if ! docker network ls | grep -q "yyc3-network"; then
        log_warning "yyc3-network ä¸å­˜åœ¨ï¼Œå°†è‡ªåŠ¨åˆ›å»º"
        docker network create yyc3-yxy-ai_yyc3-network || true
    fi

    # æ£€æŸ¥GPUæ”¯æŒï¼ˆå¯é€‰ï¼‰
    if command -v nvidia-smi &> /dev/null; then
        log_success "æ£€æµ‹åˆ°NVIDIA GPUæ”¯æŒ"
        nvidia-smi --query-gpu=name,memory.total,memory.used --format=csv,noheader,nounits
    else
        log_warning "æœªæ£€æµ‹åˆ°GPUæ”¯æŒï¼Œå°†ä½¿ç”¨CPUæ¨ç†"
    fi

    log_success "ä¾èµ–æ£€æŸ¥å®Œæˆ"
}

# åˆ›å»ºå¿…è¦çš„ç›®å½•
create_directories() {
    log_info "åˆ›å»ºå¿…è¦çš„ç›®å½•..."

    mkdir -p logs/ai
    mkdir -p monitoring
    mkdir -p chromadb/config
    mkdir -p redis
    mkdir -p data/knowledge

    log_success "ç›®å½•åˆ›å»ºå®Œæˆ"
}

# ç”Ÿæˆé…ç½®æ–‡ä»¶
generate_configs() {
    log_info "ç”Ÿæˆé…ç½®æ–‡ä»¶..."

    # ChromaDBé…ç½®
    cat > chromadb/config/chromadb-config.yml << 'EOF'
# ChromaDB Configuration
chroma_server:
  host: 0.0.0.0
  port: 8000
  cors_allow_origins: "*"
  cors_allow_credentials: true
  log_level: "INFO"

chroma_db:
  persist_directory: "/chroma/chroma"
  allow_reset: true

embedding_function:
  provider: "sentence-transformers"
  model_name: "all-MiniLM-L6-v2"
EOF

    # Redisé…ç½®
    cat > redis/ai-redis.conf << 'EOF'
# Redis Configuration for AI Services
bind 0.0.0.0
port 6379
timeout 0
keepalive 300

# Memory management
maxmemory 512mb
maxmemory-policy allkeys-lru

# Persistence
save 900 1
save 300 10
save 60 10000

# Logging
loglevel notice
logfile /var/log/redis/redis-server.log

# Security
protected-mode no
EOF

    log_success "é…ç½®æ–‡ä»¶ç”Ÿæˆå®Œæˆ"
}

# éƒ¨ç½²æœ¬åœ°AIæœåŠ¡
deploy_services() {
    local env=${1:-$DEFAULT_ENVIRONMENT}
    log_step "éƒ¨ç½²æœ¬åœ°AIæœåŠ¡ (ç¯å¢ƒ: $env)..."

    # è®¾ç½®ç¯å¢ƒå˜é‡
    export OLLAMA_PORT=${OLLAMA_PORT:-11434}
    export CHROMA_PORT=${CHROMA_PORT:-8000}
    export LOCAL_AI_PORT=${LOCAL_AI_PORT:-8081}
    export EMBEDDING_PORT=${EMBEDDING_PORT:-8082}
    export AI_REDIS_PORT=${AI_REDIS_PORT:-6380}
    export AI_PROMETHEUS_PORT=${AI_PROMETHEUS_PORT:-9091}
    export AI_GRAFANA_PORT=${AI_GRAFANA_PORT:-3002}

    # å¯åŠ¨åŸºç¡€æœåŠ¡
    if [ "$env" = "development" ]; then
        log_info "å¯åŠ¨å¼€å‘ç¯å¢ƒæœåŠ¡..."
        docker-compose -f docker-compose.ollama.yml --profile development up -d
    else
        log_info "å¯åŠ¨ç”Ÿäº§ç¯å¢ƒæœåŠ¡..."
        docker-compose -f docker-compose.ollama.yml up -d
    fi

    log_success "æœåŠ¡éƒ¨ç½²å®Œæˆ"
}

# ç­‰å¾…æœåŠ¡å¯åŠ¨
wait_for_services() {
    log_info "ç­‰å¾…æœåŠ¡å¯åŠ¨..."

    # ç­‰å¾…Ollama
    log_info "ç­‰å¾…OllamaæœåŠ¡å¯åŠ¨..."
    timeout 120 bash -c 'until curl -f http://localhost:11434/api/tags > /dev/null 2>&1; do sleep 5; done'
    if [ $? -eq 0 ]; then
        log_success "âœ“ OllamaæœåŠ¡å·²å¯åŠ¨"
    else
        log_error "âœ— OllamaæœåŠ¡å¯åŠ¨è¶…æ—¶"
        return 1
    fi

    # ç­‰å¾…ChromaDB
    log_info "ç­‰å¾…ChromaDBæœåŠ¡å¯åŠ¨..."
    timeout 60 bash -c 'until curl -f http://localhost:8000/api/v1/heartbeat > /dev/null 2>&1; do sleep 3; done'
    if [ $? -eq 0 ]; then
        log_success "âœ“ ChromaDBæœåŠ¡å·²å¯åŠ¨"
    else
        log_error "âœ— ChromaDBæœåŠ¡å¯åŠ¨è¶…æ—¶"
        return 1
    fi

    # ç­‰å¾…æœ¬åœ°AIç½‘å…³
    log_info "ç­‰å¾…æœ¬åœ°AIç½‘å…³å¯åŠ¨..."
    timeout 60 bash -c 'until curl -f http://localhost:8081/health > /dev/null 2>&1; do sleep 3; done'
    if [ $? -eq 0 ]; then
        log_success "âœ“ æœ¬åœ°AIç½‘å…³å·²å¯åŠ¨"
    else
        log_error "âœ— æœ¬åœ°AIç½‘å…³å¯åŠ¨è¶…æ—¶"
        return 1
    fi

    log_success "æ‰€æœ‰æœåŠ¡å¯åŠ¨å®Œæˆ"
}

# åˆå§‹åŒ–é»˜è®¤æ¨¡å‹
init_default_model() {
    log_step "åˆå§‹åŒ–é»˜è®¤AIæ¨¡å‹..."

    # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ¨¡å‹
    local models=$(curl -s http://localhost:11434/api/tags | grep -o '"name":"[^"]*"' | cut -d'"' -f4 || echo "")

    if [ -z "$models" ]; then
        log_info "æœªæ£€æµ‹åˆ°æ¨¡å‹ï¼Œå¼€å§‹ä¸‹è½½é»˜è®¤æ¨¡å‹: $DEFAULT_OLLAMA_MODEL"
        docker exec yyc3-ollama ollama pull "$DEFAULT_OLLAMA_MODEL"

        if [ $? -eq 0 ]; then
            log_success "âœ“ é»˜è®¤æ¨¡å‹ä¸‹è½½å®Œæˆ"
        else
            log_error "âœ— é»˜è®¤æ¨¡å‹ä¸‹è½½å¤±è´¥"
            return 1
        fi
    else
        log_success "âœ“ æ£€æµ‹åˆ°ç°æœ‰æ¨¡å‹: $(echo $models | tr '\n' ' ')"
    fi
}

# æ˜¾ç¤ºæœåŠ¡çŠ¶æ€
show_status() {
    log_info "æœ¬åœ°AIæœåŠ¡çŠ¶æ€:"
    echo ""

    # DockeræœåŠ¡çŠ¶æ€
    echo "ğŸ³ DockeræœåŠ¡:"
    docker-compose -f docker-compose.ollama.yml ps
    echo ""

    # æœåŠ¡è®¿é—®åœ°å€
    echo "ğŸŒ æœåŠ¡è®¿é—®åœ°å€:"
    echo "ğŸ¤– Ollama API:      http://localhost:11434"
    echo "ğŸ” ChromaDB:       http://localhost:8000"
    echo "ğŸšª AIç½‘å…³:         http://localhost:8081"
    echo "ğŸ“Š AIç›‘æ§é¢æ¿:     http://localhost:3002 (admin/aiadmin123)"

    if docker ps --format "table {{.Names}}" | grep -q "yyc3-ollama-webui"; then
        echo "ğŸ¨ Ollama WebUI:   http://localhost:3003"
    fi

    if docker ps --format "table {{.Names}}" | grep -q "yyc3-chromadb-admin"; then
        echo "ğŸ—‚ï¸ ChromaDBç®¡ç†:  http://localhost:8001"
    fi

    echo ""

    # å¥åº·æ£€æŸ¥
    echo "ğŸ¥ æœåŠ¡å¥åº·çŠ¶æ€:"

    # Ollamaå¥åº·æ£€æŸ¥
    if curl -f http://localhost:11434/api/tags > /dev/null 2>&1; then
        echo "âœ“ Ollama: å¥åº·"
    else
        echo "âœ— Ollama: ä¸å¯ç”¨"
    fi

    # ChromaDBå¥åº·æ£€æŸ¥
    if curl -f http://localhost:8000/api/v1/heartbeat > /dev/null 2>&1; then
        echo "âœ“ ChromaDB: å¥åº·"
    else
        echo "âœ— ChromaDB: ä¸å¯ç”¨"
    fi

    # AIç½‘å…³å¥åº·æ£€æŸ¥
    if curl -f http://localhost:8081/health > /dev/null 2>&1; then
        echo "âœ“ AIç½‘å…³: å¥åº·"
    else
        echo "âœ— AIç½‘å…³: ä¸å¯ç”¨"
    fi

    # æ¨¡å‹åˆ—è¡¨
    echo ""
    echo "ğŸ¤– å¯ç”¨æ¨¡å‹:"
    local models=$(curl -s http://localhost:11434/api/tags 2>/dev/null | grep -o '"name":"[^"]*"' | cut -d'"' -f4 || echo "æ— ")
    if [ "$models" = "æ— " ]; then
        echo "  æ— å¯ç”¨æ¨¡å‹ï¼Œè¯·å…ˆä¸‹è½½æ¨¡å‹"
    else
        echo "$models" | sed 's/^/  - /'
    fi
}

# æ¨¡å‹ç®¡ç†
manage_models() {
    local command=${1:-list}
    local model=${2:-}

    case $command in
        "list")
            log_info "åˆ—å‡ºå¯ç”¨æ¨¡å‹:"
            curl -s http://localhost:11434/api/tags | grep -o '"name":"[^"]*"' | cut -d'"' -f4 || echo "æ— å¯ç”¨æ¨¡å‹"
            ;;
        "pull")
            if [ -z "$model" ]; then
                log_error "è¯·æŒ‡å®šæ¨¡å‹åç§°"
                echo "å¯ç”¨æ¨¡å‹:"
                echo "  llama3.1:8b"
                echo "  qwen2.5:7b"
                echo "  phi3.5:3.8b"
                echo "  gemma2:7b"
                return 1
            fi
            log_info "ä¸‹è½½æ¨¡å‹: $model"
            docker exec yyc3-ollama ollama pull "$model"
            ;;
        "delete")
            if [ -z "$model" ]; then
                log_error "è¯·æŒ‡å®šæ¨¡å‹åç§°"
                return 1
            fi
            log_info "åˆ é™¤æ¨¡å‹: $model"
            docker exec yyc3-ollama ollama rm "$model"
            ;;
        "switch")
            if [ -z "$model" ]; then
                log_error "è¯·æŒ‡å®šæ¨¡å‹åç§°"
                return 1
            fi
            log_info "åˆ‡æ¢æ¨¡å‹åˆ°: $model"
            # è¿™é‡Œéœ€è¦è°ƒç”¨AIç½‘å…³çš„APIæ¥åˆ‡æ¢æ¨¡å‹
            curl -X POST http://localhost:8081/api/models/switch \
                -H "Content-Type: application/json" \
                -d "{\"model\":\"$model\"}"
            ;;
        *)
            log_error "æœªçŸ¥å‘½ä»¤: $command"
            echo "å¯ç”¨å‘½ä»¤: list, pull, delete, switch"
            return 1
            ;;
    esac
}

# æŸ¥çœ‹æ—¥å¿—
show_logs() {
    local service=${1:-all}

    case $service in
        "ollama")
            docker-compose -f docker-compose.ollama.yml logs -f ollama
            ;;
        "chromadb")
            docker-compose -f docker-compose.ollama.yml logs -f chromadb
            ;;
        "gateway")
            docker-compose -f docker-compose.ollama.yml logs -f local-ai-gateway
            ;;
        "all")
            docker-compose -f docker-compose.ollama.yml logs -f
            ;;
        *)
            log_error "æœªçŸ¥æœåŠ¡: $service"
            echo "å¯ç”¨æœåŠ¡: ollama, chromadb, gateway, all"
            return 1
            ;;
    esac
}

# å¥åº·æ£€æŸ¥
health_check() {
    log_info "æ‰§è¡Œå¥åº·æ£€æŸ¥..."

    local services=("ollama:11434/api/tags" "chromadb:8000/api/v1/heartbeat" "gateway:8081/health")
    local all_healthy=true

    for service in "${services[@]}"; do
        local name=$(echo $service | cut -d: -f1)
        local port_path=$(echo $service | cut -d: -f2-)
        local url="http://localhost:$port_path"

        if curl -f "$url" > /dev/null 2>&1; then
            log_success "âœ“ $name: å¥åº·"
        else
            log_error "âœ— $name: ä¸å¥åº·"
            all_healthy=false
        fi
    done

    if [ "$all_healthy" = true ]; then
        log_success "ğŸ‰ æ‰€æœ‰æœåŠ¡å¥åº·ï¼"
        return 0
    else
        log_error "âŒ éƒ¨åˆ†æœåŠ¡ä¸å¥åº·"
        return 1
    fi
}

# åœæ­¢æœåŠ¡
stop_services() {
    log_info "åœæ­¢æœ¬åœ°AIæœåŠ¡..."
    docker-compose -f docker-compose.ollama.yml down
    log_success "æœåŠ¡å·²åœæ­¢"
}

# æ¸…ç†æœåŠ¡
cleanup_services() {
    log_info "æ¸…ç†æœ¬åœ°AIæœåŠ¡..."
    docker-compose -f docker-compose.ollama.yml down -v
    docker system prune -f
    log_success "æ¸…ç†å®Œæˆ"
}

# é‡å¯æœåŠ¡
restart_services() {
    log_info "é‡å¯æœ¬åœ°AIæœåŠ¡..."
    docker-compose -f docker-compose.ollama.yml restart
    log_success "æœåŠ¡é‡å¯å®Œæˆ"
}

# æ›´æ–°æœåŠ¡
update_services() {
    log_info "æ›´æ–°æœ¬åœ°AIæœåŠ¡..."
    docker-compose -f docker-compose.ollama.yml pull
    docker-compose -f docker-compose.ollama.yml up -d
    log_success "æœåŠ¡æ›´æ–°å®Œæˆ"
}

# ä¸»å‡½æ•°
main() {
    local command=${1:-help}
    local env_or_option=${2:-$DEFAULT_ENVIRONMENT}

    echo "ğŸ¤– YYCÂ³ æœ¬åœ°AIæœåŠ¡éƒ¨ç½²ç®¡ç†"
    echo "================================="

    case $command in
        "deploy")
            check_dependencies
            create_directories
            generate_configs
            deploy_services "$env_or_option"
            wait_for_services
            init_default_model
            show_status
            ;;
        "update")
            update_services
            wait_for_services
            show_status
            ;;
        "stop")
            stop_services
            ;;
        "restart")
            restart_services
            wait_for_services
            show_status
            ;;
        "status")
            show_status
            ;;
        "logs")
            show_logs "$env_or_option"
            ;;
        "models")
            manage_models "$env_or_option" "${3:-}"
            ;;
        "monitor")
            log_info "æ‰“å¼€ç›‘æ§é¢æ¿: http://localhost:3002"
            if command -v open &> /dev/null; then
                open http://localhost:3002
            elif command -v xdg-open &> /dev/null; then
                xdg-open http://localhost:3002
            fi
            ;;
        "health")
            health_check
            ;;
        "cleanup")
            cleanup_services
            ;;
        "help"|"-h"|"--help")
            show_help
            ;;
        *)
            log_error "æœªçŸ¥å‘½ä»¤: $command"
            show_help
            exit 1
            ;;
    esac
}

# æ‰§è¡Œä¸»å‡½æ•°
main "$@"