#!/bin/bash

# YYCÂ³ æ•°æ®åˆ†æå¹³å°éƒ¨ç½²è„šæœ¬
# æ”¯æŒKafkaã€Flinkã€ClickHouseã€æ•°æ®åˆ†ææœåŠ¡ã€å¯è§†åŒ–æœåŠ¡çš„ä¸€é”®éƒ¨ç½²

set -euo pipefail

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# æ—¥å¿—å‡½æ•°
log_info() {
    echo -e "${GREEN}[INFO]${NC} $(date '+%Y-%m-%d %H:%M:%S') - $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $(date '+%Y-%m-%d %H:%M:%S') - $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $(date '+%Y-%m-%d %H:%M:%S') - $1"
}

log_step() {
    echo -e "${BLUE}[STEP]${NC} $(date '+%Y-%m-%d %H:%M:%S') - $1"
}

# é…ç½®å˜é‡
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$(dirname "$SCRIPT_DIR")"
COMPOSE_FILE="$PROJECT_DIR/docker-compose.data-analytics.yml"
COMPOSE_FILE_MICROSERVICES="$PROJECT_DIR/docker-compose.microservices.yml"
COMPOSE_PROFILE="${1:-full}"

# æœåŠ¡ç«¯å£æ˜ å°„
declare -A SERVICE_PORTS=(
    ["kafka"]="9092"
    ["kafka-ui"]="8080"
    ["flink-jobmanager"]="8081"
    ["clickhouse"]="8123"
    ["elasticsearch"]="9200"
    ["kibana"]="5601"
    ["realtime-analytics"]="8101"
    ["analytics-report"]="8102"
    ["prediction"]="8103"
    ["business-insights"]="8104"
    ["event-collector"]="8105"
    ["analytics-dashboard"]="3100"
    ["superset"]="8088"
    ["redis-stream"]="6380"
)

# æ£€æŸ¥ç³»ç»Ÿä¾èµ–
check_system_dependencies() {
    log_step "æ£€æŸ¥ç³»ç»Ÿä¾èµ–..."

    # æ£€æŸ¥Docker
    if ! command -v docker &> /dev/null; then
        log_error "Dockeræœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…Docker"
        exit 1
    fi

    # æ£€æŸ¥Docker Compose
    if ! command -v docker-compose &> /dev/null && ! docker compose version &> /dev/null; then
        log_error "Docker Composeæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…Docker Compose"
        exit 1
    fi

    # æ£€æŸ¥ç³»ç»Ÿèµ„æº
    local total_memory=$(free -m | awk 'NR==2{printf "%.0f", $2}')
    local available_disk=$(df -BG / | awk 'NR==2{print $4}' | sed 's/G//')

    if [ "$total_memory" -lt 8192 ]; then
        log_warn "ç³»ç»Ÿå†…å­˜ä¸è¶³8GBï¼Œå¯èƒ½å½±å“å¤§æ•°æ®åˆ†æå¹³å°æ€§èƒ½"
    fi

    if [ "$available_disk" -lt 50 ]; then
        log_warn "ç£ç›˜å¯ç”¨ç©ºé—´ä¸è¶³50GBï¼Œå¯èƒ½å½±å“æ•°æ®å­˜å‚¨"
    fi

    # æ£€æŸ¥ç«¯å£å ç”¨
    log_info "æ£€æŸ¥ç«¯å£å ç”¨æƒ…å†µ..."
    local occupied_ports=()

    for service in "${!SERVICE_PORTS[@]}"; do
        local port="${SERVICE_PORTS[$service]}"
        if lsof -Pi :$port -sTCP:LISTEN -t >/dev/null 2>&1; then
            occupied_ports+=("$port ($service)")
        fi
    done

    if [ ${#occupied_ports[@]} -gt 0 ]; then
        log_warn "ä»¥ä¸‹ç«¯å£å·²è¢«å ç”¨ï¼š"
        for port in "${occupied_ports[@]}"; do
            echo "  - $port"
        done
        read -p "æ˜¯å¦ç»§ç»­éƒ¨ç½²ï¼Ÿ(y/N): " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            log_info "éƒ¨ç½²å·²å–æ¶ˆ"
            exit 0
        fi
    fi

    log_info "ç³»ç»Ÿä¾èµ–æ£€æŸ¥å®Œæˆ"
}

# ç¯å¢ƒå‡†å¤‡
prepare_environment() {
    log_step "å‡†å¤‡æ•°æ®åˆ†æå¹³å°ç¯å¢ƒ..."

    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    mkdir -p "$PROJECT_DIR/analytics/services/realtime-analytics/logs"
    mkdir -p "$PROJECT_DIR/analytics/services/analytics-report/logs"
    mkdir -p "$PROJECT_DIR/analytics/services/prediction/logs"
    mkdir -p "$PROJECT_DIR/analytics/services/business-insights/logs"
    mkdir -p "$PROJECT_DIR/analytics/collectors/event-collector/logs"
    mkdir -p "$PROJECT_DIR/analytics/dashboard/logs"
    mkdir -p "$PROJECT_DIR/analytics/clickhouse/config"
    mkdir -p "$PROJECT_DIR/analytics/clickhouse/init"
    mkdir -p "$PROJECT_DIR/analytics/flink/jobs"
    mkdir -p "$PROJECT_DIR/analytics/superset/config"
    mkdir -p "$PROJECT_DIR/analytics/superset/dashboards"
    mkdir -p "$PROJECT_DIR/analytics/reports/templates"
    mkdir -p "$PROJECT_DIR/analytics/reports/output"

    # åˆ›å»ºClickHouseé…ç½®
    cat > "$PROJECT_DIR/analytics/clickhouse/config/config.xml" << 'EOF'
<?xml version="1.0"?>
<yandex>
    <logger>
        <level>information</level>
        <console>true</console>
    </logger>

    <http_port>8123</http_port>
    <tcp_port>9000</tcp_port>
    <mysql_port>9004</mysql_port>

    <listen_host>::</listen_host>
    <listen_host>0.0.0.0</listen_host>

    <max_connections>4096</max_connections>
    <keep_alive_timeout>3</keep_alive_timeout>
    <max_concurrent_queries>100</max_concurrent_queries>

    <uncompressed_cache_size>8589934592</uncompressed_cache_size>
    <mark_cache_size>5368709120</mark_cache_size>

    <path>/var/lib/clickhouse/</path>
    <tmp_path>/var/lib/clickhouse/tmp/</tmp_path>
    <user_files_path>/var/lib/clickhouse/user_files/</user_files_path>

    <users_config>users.xml</users_config>
    <default_profile>default</default_profile>
    <default_database>default</default_database>

    <timezone>Asia/Shanghai</timezone>

    <remote_servers incl="clickhouse_remote_servers" />
    <zookeeper incl="zookeeper-servers" optional="true" />
    <macros incl="macros" optional="true" />

    <builtin_dictionaries_reload_interval>3600</builtin_dictionaries_reload_interval>

    <max_session_timeout>3600</max_session_timeout>
    <default_session_timeout>60</default_session_timeout>
</yandex>
EOF

    # åˆ›å»ºClickHouseç”¨æˆ·é…ç½®
    cat > "$PROJECT_DIR/analytics/clickhouse/config/users.xml" << 'EOF'
<?xml version="1.0"?>
<yandex>
    <profiles>
        <default>
            <max_memory_usage>10000000000</max_memory_usage>
            <use_uncompressed_cache>0</use_uncompressed_cache>
            <load_balancing>random</load_balancing>
        </default>
        <readonly>
            <max_memory_usage>10000000000</max_memory_usage>
            <use_uncompressed_cache>0</use_uncompressed_cache>
            <load_balancing>random</load_balancing>
            <readonly>1</readonly>
        </readonly>
    </profiles>

    <users>
        <default>
            <password></password>
            <networks incl="networks" replace="replace">
                <ip>::/0</ip>
            </networks>
            <profile>default</profile>
            <quota>default</quota>
            <databases>
                <database_name>
                    <filter_expression>.*</filter_expression>
                </database_name>
            </databases>
        </default>
        <yyc3>
            <password>analytics_password</password>
            <networks incl="networks" replace="replace">
                <ip>::/0</ip>
            </networks>
            <profile>default</profile>
            <quota>default</quota>
            <databases>
                <yyc3_analytics>
                    <filter_expression>.*</filter_expression>
                </yyc3_analytics>
            </databases>
        </yyc3>
    </users>

    <networks>
        <ip>::/0</ip>
    </networks>
</yandex>
EOF

    # åˆ›å»ºClickHouseåˆå§‹åŒ–è„šæœ¬
    cat > "$PROJECT_DIR/analytics/clickhouse/init/01-init-database.sql" << 'EOF'
-- åˆ›å»ºYYCÂ³åˆ†ææ•°æ®åº“
CREATE DATABASE IF NOT EXISTS yyc3_analytics;

-- ä½¿ç”¨æ•°æ®åº“
USE yyc3_analytics;

-- ç”¨æˆ·è¡Œä¸ºäº‹ä»¶è¡¨
CREATE TABLE IF NOT EXISTS user_events_local (
    event_uuid UUID,
    user_id String,
    session_id String,
    event_type Enum8('click' = 1, 'view' = 2, 'search' = 3, 'chat' = 4, 'page_view' = 5),
    event_timestamp DateTime,
    properties Map(String, String),
    source String,
    created_date Date MATERIALIZED toDate(event_timestamp)
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(created_date)
ORDER BY (user_id, event_timestamp, event_type)
TTL created_date + INTERVAL 90 DAY;

-- AIå¯¹è¯è¡¨
CREATE TABLE IF NOT EXISTS ai_conversations_local (
    conversation_id UUID,
    user_id String,
    message_type Enum8('user' = 1, 'assistant' = 2),
    content String,
    sentiment_score Float32,
    topics Array(String),
    response_time_ms UInt32,
    satisfaction_score Float32,
    timestamp DateTime,
    created_date Date MATERIALIZED toDate(timestamp)
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(created_date)
ORDER BY (conversation_id, timestamp);

-- æˆé•¿è®°å½•è¡¨
CREATE TABLE IF NOT EXISTS growth_updates_local (
    update_id UUID,
    user_id String,
    growth_type Enum8('learning_time' = 1, 'skill_improvement' = 2, 'milestone' = 3),
    improvement_value Float32,
    previous_value Float32,
    timestamp DateTime,
    created_date Date MATERIALIZED toDate(timestamp)
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(created_date)
ORDER BY (user_id, growth_type, timestamp);

-- æ¨èåé¦ˆè¡¨
CREATE TABLE IF NOT EXISTS recommendation_feedback_local (
    feedback_id UUID,
    user_id String,
    recommendation_id String,
    rating UInt8,
    feedback_text String,
    timestamp DateTime,
    created_date Date MATERIALIZED toDate(timestamp)
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(created_date)
ORDER BY (user_id, recommendation_id, timestamp);

-- ç³»ç»ŸæŒ‡æ ‡è¡¨
CREATE TABLE IF NOT EXISTS system_metrics_local (
    metric_id UUID,
    metric_name String,
    metric_value Float64,
    threshold Float64,
    unit String,
    timestamp DateTime,
    created_date Date MATERIALIZED toDate(timestamp)
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(created_date)
ORDER BY (metric_name, timestamp);
EOF

    # åˆ›å»ºFlinkä½œä¸šé…ç½®
    cat > "$PROJECT_DIR/analytics/flink/jobs/data-analytics-job.properties" << 'EOF'
# Flinkä½œä¸šé…ç½®
flink.job.name: YYC3-Data-Analytics-Job
flink.parallelism: 4
flink.checkpoint.interval: 60000
flink.checkpoint.timeout: 300000
flink.restart-strategy: fixed-delay
flink.restart-strategy.fixed-delay.attempts: 3
flink.restart-strategy.fixed-delay.delay: 10000
EOF

    # åˆ›å»ºSuperseté…ç½®
    cat > "$PROJECT_DIR/analytics/superset/config/superset_config.py" << 'EOF'
from celery.schedules import crontab

# Superseté…ç½®
FEATURE_FLAGS = {
    'ALERT_REPORTS': True,
    'DASHBOARD_RBAC': True,
    'ENABLE_CHART_ASYNC_EXPORT': True,
    'DYNAMIC_PLUGINS': True,
}

# æ•°æ®åº“é…ç½®
SQLALCHEMY_DATABASE_URI = 'clickhouse+yyc3:yyc3:analytics_password@clickhouse:9000/yyc3_analytics'

# Redisé…ç½®
REDIS_URL = 'redis://redis-stream:6379/0'

# Celeryé…ç½®
CELERY_CONFIG = {
    'BROKER_URL': 'redis://redis-stream:6379/1',
    'RESULT_BACKEND': 'redis://redis-stream:6379/2',
    'CELERYBEAT_SCHEDULE': {
        'analytics.daily_report': {
            'task': 'analytics.reports.daily',
            'schedule': crontab(hour=8, minute=0),
        },
    }
}

# é‚®ä»¶é…ç½®
SMTP_HOST = os.environ.get('SMTP_HOST', 'localhost')
SMTP_PORT = int(os.environ.get('SMTP_PORT', 587))
SMTP_USER = os.environ.get('SMTP_USER', '')
SMTP_PASSWORD = os.environ.get('SMTP_PASS', '')
SMTP_MAIL_FROM = os.environ.get('SMTP_MAIL_FROM', 'noreply@yyc3.app')

# å®‰å…¨é…ç½®
SECRET_KEY = os.environ.get('SUPERSET_SECRET_KEY', 'yyc3-superset-secret-key-2023')
ENABLE_PROXY_FIX = True
ENABLE_CORS = True
CORS_ALLOW_ORIGIN = ['http://localhost:3100', 'https://yyc3.app']
EOF

    log_info "ç¯å¢ƒå‡†å¤‡å®Œæˆ"
}

# æ„å»ºæ•°æ®åˆ†ææœåŠ¡
build_analytics_services() {
    log_step "æ„å»ºæ•°æ®åˆ†ææœåŠ¡..."

    local services=(
        "analytics/services/realtime-analytics"
        "analytics/services/analytics-report"
        "analytics/services/prediction"
        "analytics/services/business-insights"
        "analytics/collectors/event-collector"
        "analytics/dashboard"
    )

    for service in "${services[@]}"; do
        if [ -f "$PROJECT_DIR/$service/Dockerfile" ]; then
            log_info "æ„å»ºæœåŠ¡: $service"
            cd "$PROJECT_DIR/$service"
            docker build -t "yyc3-$(basename $service)" .
            cd "$PROJECT_DIR"
        else
            log_warn "Dockerfileä¸å­˜åœ¨ï¼Œè·³è¿‡: $service"
        fi
    done

    log_info "æ•°æ®åˆ†ææœåŠ¡æ„å»ºå®Œæˆ"
}

# éƒ¨ç½²æ•°æ®åˆ†æå¹³å°
deploy_analytics_platform() {
    log_step "éƒ¨ç½²æ•°æ®åˆ†æå¹³å°..."

    cd "$PROJECT_DIR"

    # æ ¹æ®é…ç½®æ–‡ä»¶é€‰æ‹©éƒ¨ç½²çš„é…ç½®æ–‡ä»¶
    local compose_cmd="docker-compose"
    if docker compose version &> /dev/null; then
        compose_cmd="docker compose"
    fi

    # å…ˆå¯åŠ¨å¾®æœåŠ¡åŸºç¡€æ¶æ„
    if [ -f "$COMPOSE_FILE_MICROSERVICES" ]; then
        log_info "å¯åŠ¨å¾®æœåŠ¡åŸºç¡€æ¶æ„..."
        $compose_cmd -f "$COMPOSE_FILE_MICROSERVICES" up -d consul postgres redis neo4j
        sleep 30
    fi

    # éƒ¨ç½²æ•°æ®åˆ†æåŸºç¡€è®¾æ–½
    log_info "éƒ¨ç½²æ•°æ®åˆ†æåŸºç¡€è®¾æ–½..."
    $compose_cmd -f "$COMPOSE_FILE" up -d zookeeper kafka

    # ç­‰å¾…Kafkaå°±ç»ª
    log_info "ç­‰å¾…Kafkaå¯åŠ¨..."
    sleep 30

    # éƒ¨ç½²æ•°æ®å¤„ç†å±‚
    log_info "éƒ¨ç½²æ•°æ®å¤„ç†å±‚..."
    $compose_cmd -f "$COMPOSE_FILE" up -d flink-jobmanager flink-taskmanager clickhouse redis-stream elasticsearch

    # ç­‰å¾…æ•°æ®å¤„ç†å±‚å°±ç»ª
    log_info "ç­‰å¾…æ•°æ®å¤„ç†æœåŠ¡å¯åŠ¨..."
    sleep 45

    # éƒ¨ç½²åˆ†ææœåŠ¡
    log_info "éƒ¨ç½²åˆ†ææœåŠ¡..."
    $compose_cmd -f "$COMPOSE_FILE" up -d \
        event-collector \
        realtime-analytics-service \
        analytics-report-service \
        prediction-service \
        business-insights-service

    # ç­‰å¾…åˆ†ææœåŠ¡å°±ç»ª
    log_info "ç­‰å¾…åˆ†ææœåŠ¡å¯åŠ¨..."
    sleep 30

    # éƒ¨ç½²å¯è§†åŒ–å±‚
    log_info "éƒ¨ç½²å¯è§†åŒ–å±‚..."
    $compose_cmd -f "$COMPOSE_FILE" up -d kafka-ui kibana analytics-dashboard superset

    log_info "æ•°æ®åˆ†æå¹³å°éƒ¨ç½²å®Œæˆ"
}

# åˆå§‹åŒ–æ•°æ®
initialize_data() {
    log_step "åˆå§‹åŒ–æ•°æ®åˆ†æå¹³å°..."

    # ç­‰å¾…ClickHouseå°±ç»ª
    log_info "ç­‰å¾…ClickHouseå°±ç»ª..."
    local clickhouse_ready=false
    for i in {1..30}; do
        if curl -f http://localhost:8123/ping &>/dev/null; then
            clickhouse_ready=true
            break
        fi
        sleep 2
    done

    if [ "$clickhouse_ready" = true ]; then
        log_info "ClickHouseå·²å°±ç»ªï¼Œæ‰§è¡Œåˆå§‹åŒ–è„šæœ¬..."

        # åˆ›å»ºæ•°æ®åº“å’Œè¡¨
        curl -X POST http://localhost:8123 --data "CREATE DATABASE IF NOT EXISTS yyc3_analytics"

        # æ‰§è¡Œåˆå§‹åŒ–SQL
        if [ -f "$PROJECT_DIR/analytics/clickhouse/init/01-init-database.sql" ]; then
            curl -X POST http://localhost:8123 \
                 -H "Content-Type: text/plain" \
                 --data-binary @"$PROJECT_DIR/analytics/clickhouse/init/01-init-database.sql"
        fi

        log_info "ClickHouseåˆå§‹åŒ–å®Œæˆ"
    else
        log_error "ClickHouseå¯åŠ¨è¶…æ—¶"
    fi

    # åˆå§‹åŒ–Superset
    log_info "åˆå§‹åŒ–Superset..."
    if docker ps | grep -q yyc3-superset; then
        # åˆ›å»ºç®¡ç†å‘˜ç”¨æˆ·
        docker exec yyc3-superset superset fab create-admin \
            --username admin \
            --firstname Admin \
            --lastname User \
            --email admin@yyc3.app \
            --password admin

        # åˆå§‹åŒ–æ•°æ®åº“
        docker exec yyc3-superset superset db upgrade

        # åˆ›å»ºç¤ºä¾‹æ•°æ®å’Œè§’è‰²
        docker exec yyc3-superset superset init

        log_info "Supersetåˆå§‹åŒ–å®Œæˆ"
    fi

    log_info "æ•°æ®åˆå§‹åŒ–å®Œæˆ"
}

# éªŒè¯éƒ¨ç½²
verify_deployment() {
    log_step "éªŒè¯æ•°æ®åˆ†æå¹³å°éƒ¨ç½²..."

    local compose_cmd="docker-compose"
    if docker compose version &> /dev/null; then
        compose_cmd="docker compose"
    fi

    # æ£€æŸ¥æœåŠ¡çŠ¶æ€
    log_info "æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€..."
    local failed_services=()

    # æ ¸å¿ƒæœåŠ¡å¥åº·æ£€æŸ¥
    local services=(
        "kafka:9092"
        "clickhouse:8123"
        "flink-jobmanager:8081"
        "elasticsearch:9200"
        "realtime-analytics:8101"
        "analytics-dashboard:3100"
    )

    for service in "${services[@]}"; do
        local service_name="${service%%:*}"
        local port="${service##*:}"

        log_info "æ£€æŸ¥ $service_name (ç«¯å£ $port)..."

        case $service_name in
            "kafka")
                if docker exec yyc3-kafka kafka-broker-api-versions --bootstrap-server localhost:9092 &>/dev/null; then
                    log_info "âœ… $service_name è¿è¡Œæ­£å¸¸"
                else
                    failed_services+=("$service_name")
                    log_error "âŒ $service_name å¥åº·æ£€æŸ¥å¤±è´¥"
                fi
                ;;
            "clickhouse")
                if curl -f http://localhost:$port/ping &>/dev/null; then
                    log_info "âœ… $service_name è¿è¡Œæ­£å¸¸"
                else
                    failed_services+=("$service_name")
                    log_error "âŒ $service_name å¥åº·æ£€æŸ¥å¤±è´¥"
                fi
                ;;
            "flink-jobmanager")
                if curl -f http://localhost:$port &>/dev/null; then
                    log_info "âœ… $service_name è¿è¡Œæ­£å¸¸"
                else
                    failed_services+=("$service_name")
                    log_error "âŒ $service_name å¥åº·æ£€æŸ¥å¤±è´¥"
                fi
                ;;
            "elasticsearch")
                if curl -f http://localhost:$port/_cluster/health &>/dev/null; then
                    log_info "âœ… $service_name è¿è¡Œæ­£å¸¸"
                else
                    failed_services+=("$service_name")
                    log_error "âŒ $service_name å¥åº·æ£€æŸ¥å¤±è´¥"
                fi
                ;;
            *)
                if curl -f http://localhost:$port/health &>/dev/null; then
                    log_info "âœ… $service_name è¿è¡Œæ­£å¸¸"
                else
                    failed_services+=("$service_name")
                    log_error "âŒ $service_name å¥åº·æ£€æŸ¥å¤±è´¥"
                fi
                ;;
        esac
    done

    if [ ${#failed_services[@]} -gt 0 ]; then
        log_warn "ä»¥ä¸‹æœåŠ¡æœªå°±ç»ªï¼š"
        for service in "${failed_services[@]}"; do
            echo "  - $service"
        done
        log_warn "è¯·æ£€æŸ¥æœåŠ¡æ—¥å¿—ï¼š"
        echo "  $compose_cmd -f $COMPOSE_FILE logs [service-name]"
    else
        log_info "ğŸ‰ æ‰€æœ‰æœåŠ¡éƒ¨ç½²æˆåŠŸï¼"
    fi

    # æ˜¾ç¤ºè®¿é—®ä¿¡æ¯
    echo
    log_info "æ•°æ®åˆ†æå¹³å°è®¿é—®åœ°å€ï¼š"
    echo "  ğŸ“Š æ•°æ®åˆ†æä»ªè¡¨æ¿:   http://localhost:3100"
    echo "  ğŸ”„ Kafka UI:        http://localhost:8080"
    echo "  âš¡ Flink UI:         http://localhost:8081"
    echo "  ğŸ—„ï¸ ClickHouse:      http://localhost:8123"
    echo "  ğŸ” Elasticsearch:  http://localhost:9200"
    echo "  ğŸ“ˆ Kibana:           http://localhost:5601"
    echo "  ğŸ¯ Superset:        http://localhost:8088 (admin/admin)"
    echo "  ğŸ“¡ å®æ—¶åˆ†æAPI:      http://localhost:8101"
    echo "  ğŸ“‹ æŠ¥è¡¨æœåŠ¡API:      http://localhost:8102"
    echo "  ğŸ”® é¢„æµ‹åˆ†æAPI:      http://localhost:8103"
    echo "  ğŸ’¡ ä¸šåŠ¡æ´å¯ŸAPI:      http://localhost:8104"
    echo "  ğŸ“¥ äº‹ä»¶æ”¶é›†å™¨:       http://localhost:8105"
    echo

    log_info "APIæœåŠ¡ç«¯ç‚¹ï¼š"
    echo "  ğŸ“Š å®æ—¶æŒ‡æ ‡:         http://localhost:8101/api/v1/realtime/metrics"
    echo "  ğŸ“ˆ è¶‹åŠ¿åˆ†æ:         http://localhost:8101/api/v1/analytics/trends"
    echo "  ğŸ” å¼‚å¸¸æ£€æµ‹:         http://localhost:8101/api/v1/analytics/anomalies"
    echo "  ğŸ“‹ ç”ŸæˆæŠ¥è¡¨:         http://localhost:8102/api/v1/reports/generate"
    echo "  ğŸ”® é¢„æµ‹åˆ†æ:         http://localhost:8103/api/v1/prediction/forecast"
    echo "  ğŸ’¡ ä¸šåŠ¡æ´å¯Ÿ:         http://localhost:8104/api/v1/insights"
}

# æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
show_help() {
    cat << EOF
YYCÂ³ æ•°æ®åˆ†æå¹³å°éƒ¨ç½²è„šæœ¬

ç”¨æ³•: $0 [COMMAND] [OPTIONS]

å‘½ä»¤:
  deploy     éƒ¨ç½²å®Œæ•´çš„æ•°æ®åˆ†æå¹³å°
  build      æ„å»ºæ‰€æœ‰åˆ†ææœåŠ¡é•œåƒ
  status     æ˜¾ç¤ºæœåŠ¡çŠ¶æ€
  logs       æ˜¾ç¤ºæœåŠ¡æ—¥å¿—
  stop       åœæ­¢æ‰€æœ‰æœåŠ¡
  restart    é‡å¯æ‰€æœ‰æœåŠ¡
  clean      æ¸…ç†æ‰€æœ‰å®¹å™¨å’Œæ•°æ®
  health     å¥åº·æ£€æŸ¥
  help       æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯

ç¤ºä¾‹:
  $0 deploy           # éƒ¨ç½²å®Œæ•´å¹³å°
  $0 status           # æŸ¥çœ‹æœåŠ¡çŠ¶æ€
  $0 logs clickhouse  # æŸ¥çœ‹ClickHouseæ—¥å¿—
  $0 restart          # é‡å¯æ‰€æœ‰æœåŠ¡
  $0 clean            # æ¸…ç†ç¯å¢ƒ

æ›´å¤šä¿¡æ¯è¯·æŸ¥çœ‹æ–‡æ¡£: PHASE2-WEEK15-16-DATA-ANALYTICS-PLAN.md
EOF
}

# ä¸»å‡½æ•°
main() {
    case "${1:-deploy}" in
        "deploy")
            log_info "å¼€å§‹éƒ¨ç½²YYCÂ³æ•°æ®åˆ†æå¹³å°..."
            check_system_dependencies
            prepare_environment
            build_analytics_services
            deploy_analytics_platform
            initialize_data
            verify_deployment
            ;;
        "build")
            log_info "æ„å»ºæ•°æ®åˆ†ææœåŠ¡..."
            build_analytics_services
            ;;
        "status")
            log_info "æ˜¾ç¤ºæœåŠ¡çŠ¶æ€..."
            cd "$PROJECT_DIR"
            docker-compose -f "$COMPOSE_FILE" ps
            ;;
        "logs")
            local service="${2:-}"
            if [ -z "$service" ]; then
                cd "$PROJECT_DIR"
                docker-compose -f "$COMPOSE_FILE" logs -f
            else
                cd "$PROJECT_DIR"
                docker-compose -f "$COMPOSE_FILE" logs -f "$service"
            fi
            ;;
        "stop")
            log_info "åœæ­¢æ‰€æœ‰æœåŠ¡..."
            cd "$PROJECT_DIR"
            docker-compose -f "$COMPOSE_FILE" down
            ;;
        "restart")
            log_info "é‡å¯æ‰€æœ‰æœåŠ¡..."
            cd "$PROJECT_DIR"
            docker-compose -f "$COMPOSE_FILE" restart
            ;;
        "clean")
            log_warn "æ¸…ç†æ‰€æœ‰å®¹å™¨å’Œæ•°æ®..."
            read -p "ç¡®å®šè¦æ¸…ç†æ‰€æœ‰æ•°æ®å—ï¼Ÿ(y/N): " -n 1 -r
            echo
            if [[ $REPLY =~ ^[Yy]$ ]]; then
                cd "$PROJECT_DIR"
                docker-compose -f "$COMPOSE_FILE" down -v --remove-orphans
                docker system prune -f
                log_info "æ¸…ç†å®Œæˆ"
            else
                log_info "æ¸…ç†å·²å–æ¶ˆ"
            fi
            ;;
        "health")
            log_info "æ‰§è¡Œå¥åº·æ£€æŸ¥..."
            verify_deployment
            ;;
        "help"|"--help"|"-h")
            show_help
            ;;
        *)
            log_error "æœªçŸ¥å‘½ä»¤: $1"
            show_help
            exit 1
            ;;
    esac
}

# è„šæœ¬å…¥å£
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi